# Documentation Index

Complete step-by-step guides for fine-tuning and deploying LLMs.

## Quick Links

- üöÄ **[Google Colab Guide](COLAB_GUIDE.md)** - Fine-tune on free GPU in your browser!
- üìì **[Colab Notebook](../notebooks/llm_finetune_colab.ipynb)** - Ready-to-run notebook
- üñ•Ô∏è **RunPod/Local Setup** - Follow phase-by-phase guides below

## Overview

This documentation is organized into 5 phases, each covering a distinct stage of the fine-tuning pipeline. Follow them in order for the best experience.

## Phase-by-Phase Guides

### [Phase 1: Environment Setup](PHASE1_SETUP.md)
‚è±Ô∏è Time: 15-20 minutes

Set up your RunPod instance or local machine with all required dependencies.

**You'll learn**:
- Creating a RunPod GPU pod
- Installing dependencies (PyTorch, Transformers, LangChain)
- Configuring environment variables
- Verifying GPU and library installations

**Outputs**:
- ‚úÖ Working development environment
- ‚úÖ GPU access confirmed
- ‚úÖ All libraries installed

---

### [Phase 2: Data Preparation](PHASE2_DATA_PREPARATION.md)
‚è±Ô∏è Time: 10 minutes - 2 hours (depending on QA generation)

Prepare your training data using LangChain document processing.

**You'll learn**:
- Formatting JSON/JSONL data
- Processing PDF/DOCX documents
- Generating Q&A pairs with Claude
- Data augmentation techniques
- Creating configuration files

**Outputs**:
- ‚úÖ Processed training data (JSONL format)
- ‚úÖ Configuration file (YAML)
- ‚úÖ Optional: Generated Q&A pairs

---

### [Phase 3: Training](PHASE3_TRAINING.md)
‚è±Ô∏è Time: 30 minutes - 2 days (depending on model size and data)

Train your model using PEFT/LoRA fine-tuning.

**You'll learn**:
- Configuring training hyperparameters
- GPU memory optimization (QLoRA, gradient checkpointing)
- Running training in tmux
- Monitoring training with TensorBoard/W&B
- Troubleshooting common issues

**Outputs**:
- ‚úÖ Trained LoRA adapter weights
- ‚úÖ Training logs and metrics
- ‚úÖ Model checkpoints

---

### [Phase 4: Evaluation & Export](PHASE4_EVALUATION.md)
‚è±Ô∏è Time: 30 minutes - 2 hours

Evaluate your fine-tuned model and export it for deployment.

**You'll learn**:
- Testing model with sample prompts
- Comparing base vs fine-tuned performance
- Merging LoRA adapter with base model
- Exporting to different formats (GGUF, ONNX)
- Benchmarking inference speed

**Outputs**:
- ‚úÖ Merged model (full weights)
- ‚úÖ Exported model (GGUF/ONNX)
- ‚úÖ Performance benchmarks
- ‚úÖ Evaluation results

---

### [Phase 5: Deployment](PHASE5_DEPLOYMENT.md)
‚è±Ô∏è Time: 1-4 hours (depending on deployment method)

Deploy your model for production use.

**You'll learn**:
- Local deployment with llama.cpp
- RunPod Serverless deployment
- HuggingFace Inference Endpoints
- vLLM for high-throughput serving
- Building custom FastAPI servers
- Production best practices

**Outputs**:
- ‚úÖ Deployed model (API endpoint)
- ‚úÖ Monitoring and logging configured
- ‚úÖ Documentation for users

---

## Quick Start Paths

### Path A: Simple JSON Fine-tuning (Fastest)
1. [Phase 1: Setup](PHASE1_SETUP.md) ‚Üí 15 min
2. [Phase 2: Data Prep](PHASE2_DATA_PREPARATION.md) (JSON) ‚Üí 5 min
3. [Phase 3: Training](PHASE3_TRAINING.md) (7B model) ‚Üí 1-3 hours
4. [Phase 4: Evaluation](PHASE4_EVALUATION.md) ‚Üí 30 min
5. [Phase 5: Deployment](PHASE5_DEPLOYMENT.md) (Local) ‚Üí 30 min

**Total**: ~3-5 hours

---

### Path B: PDF to QA Dataset
1. [Phase 1: Setup](PHASE1_SETUP.md) ‚Üí 15 min
2. [Phase 2: Data Prep](PHASE2_DATA_PREPARATION.md) (PDF + QA) ‚Üí 1-2 hours
3. [Phase 3: Training](PHASE3_TRAINING.md) ‚Üí 2-5 hours
4. [Phase 4: Evaluation](PHASE4_EVALUATION.md) ‚Üí 1 hour
5. [Phase 5: Deployment](PHASE5_DEPLOYMENT.md) (RunPod) ‚Üí 2 hours

**Total**: ~7-11 hours

---

### Path C: Production Deployment
1. [Phase 1: Setup](PHASE1_SETUP.md) ‚Üí 15 min
2. [Phase 2: Data Prep](PHASE2_DATA_PREPARATION.md) ‚Üí Variable
3. [Phase 3: Training](PHASE3_TRAINING.md) ‚Üí Variable
4. [Phase 4: Evaluation](PHASE4_EVALUATION.md) ‚Üí 2 hours
5. [Phase 5: Deployment](PHASE5_DEPLOYMENT.md) (vLLM/FastAPI) ‚Üí 4 hours

**Total**: Variable + ~7 hours

---

## Additional Resources

### Architecture & Diagrams
- [ARCHITECTURE.md](../ARCHITECTURE.md) - ASCII diagrams and technical architecture
- [DIAGRAMS.md](../DIAGRAMS.md) - Mermaid flowcharts (renders on GitHub)

### Project Files
- [README.md](../README.md) - Main project README
- [CONTRIBUTING.md](../CONTRIBUTING.md) - Contribution guidelines
- [CODE_OF_CONDUCT.md](../CODE_OF_CONDUCT.md) - Community guidelines

### Example Configs
- `configs/json_sft.yaml` - JSON supervised fine-tuning
- `configs/pdf_causal_lm.yaml` - PDF continued pretraining
- `configs/qa_generation.yaml` - PDF to Q&A generation
- `configs/vedic_astrology.yaml` - Domain-specific example

---

## Support & Community

### Getting Help

1. **Check the docs first** - Most questions are answered in these guides
2. **GitHub Issues** - Report bugs or request features
3. **Discussions** - Ask questions, share results

### Reporting Issues

When reporting issues, include:
- Phase you're on
- Full error message
- Configuration file (sanitized)
- GPU type and VRAM
- Steps to reproduce

---

## Tips for Success

### Before You Start
- [ ] Estimate total time needed
- [ ] Check GPU availability and cost
- [ ] Prepare your dataset
- [ ] Set realistic expectations

### During Training
- [ ] Use tmux to prevent interruptions
- [ ] Monitor GPU utilization
- [ ] Save checkpoints frequently
- [ ] Watch for OOM errors early

### After Training
- [ ] Test thoroughly before deploying
- [ ] Compare with base model
- [ ] Document your findings
- [ ] Plan for iteration

---

## What to Expect

### Timeline (7B Model, 10K Samples)
- **Day 1**: Setup, data prep, start training (4-6 hours active)
- **Day 2**: Training completes, evaluation (2-3 hours active)
- **Day 3**: Deployment and testing (3-4 hours active)

### Costs (RunPod, 7B Model)
- **Setup**: Free
- **Training**: $5-20 (depending on duration)
- **Deployment**: $0.44/hour (serverless cheaper)

### Expected Results
- Training loss: Should drop from ~3.0 to <1.0
- Evaluation: Model should outperform base on your domain
- Inference: 20-40 tokens/sec on consumer GPU

---

## Next Steps

Start with [Phase 1: Environment Setup](PHASE1_SETUP.md) ‚Üí

Good luck with your fine-tuning journey! üöÄ
