# Environment Variables for LLM Fine-Tuning with LangChain
# Copy this file to .env and fill in your values

# =============================================================================
# Required for Gated Models (Llama)
# =============================================================================
# Get your token at: https://huggingface.co/settings/tokens
HF_TOKEN=

# =============================================================================
# Optional: QA Generation with Claude Haiku
# =============================================================================
# Required only if using --enable-qa flag or qa_generation_enabled: true
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Alternative: OpenAI for QA Generation
# Get your key at: https://platform.openai.com/api-keys
# OPENAI_API_KEY=

# =============================================================================
# Optional: Experiment Tracking
# =============================================================================
# Get your key at: https://wandb.ai/authorize
WANDB_API_KEY=

# =============================================================================
# Model Configuration Overrides
# =============================================================================
# MODEL_NAME=Qwen/Qwen2.5-14B-Instruct

# =============================================================================
# Training Configuration Overrides
# =============================================================================
# OUTPUT_DIR=/workspace/output
# MAX_STEPS=1000
# NUM_EPOCHS=3
# BATCH_SIZE=1
# LEARNING_RATE=2e-4
# MAX_SEQ_LENGTH=2048

# =============================================================================
# Data Configuration Overrides
# =============================================================================
# DATA_PATH=/workspace/data
# INPUT_TYPE=json
# ENABLE_QA_GENERATION=false

# =============================================================================
# RunPod Configuration
# =============================================================================
# RUNPOD_VOLUME_PATH=/workspace

# Hugging Face Cache Directory
HF_HOME=/workspace/hf_cache
TRANSFORMERS_CACHE=/workspace/hf_cache

# Disable tokenizers parallelism warning
TOKENIZERS_PARALLELISM=false
