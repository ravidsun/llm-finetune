{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# LLM Fine-Tuning with LangChain on Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ravidsun/llm-finetune/blob/master/notebooks/llm_finetune_colab.ipynb)\n",
        "\n",
        "Complete notebook for fine-tuning open-source LLMs (Llama, Qwen, Mistral) using Google Colab.\n",
        "\n",
        "## Features\n",
        "- ‚úÖ Works on Colab Free (T4) and Pro+ (A100)\n",
        "- ‚úÖ QLoRA 4-bit training for memory efficiency\n",
        "- ‚úÖ LangChain document processing\n",
        "- ‚úÖ Optional QA generation with Claude\n",
        "- ‚úÖ Data augmentation\n",
        "- ‚úÖ Save to Google Drive\n",
        "\n",
        "## Requirements\n",
        "- Google account\n",
        "- GPU runtime (T4 free, A100 with Pro+)\n",
        "- Hugging Face token (for gated models)\n",
        "\n",
        "## Estimated Time\n",
        "- Setup: 5-10 minutes\n",
        "- Training (1K samples, 7B model): 30-60 minutes on T4\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n",
            "‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\n"
          ]
        }
      ],
      "source": [
        "%pip install -q torch\n",
        "\n",
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install core ML libraries (takes ~3-5 minutes)\n",
        "!pip install -q -U \\\n",
        "    torch torchvision torchaudio \\\n",
        "    transformers>=4.40.0 \\\n",
        "    datasets>=2.18.0 \\\n",
        "    accelerate>=0.27.0 \\\n",
        "    peft>=0.10.0 \\\n",
        "    trl>=0.8.0 \\\n",
        "    bitsandbytes>=0.43.0 \\\n",
        "    safetensors>=0.4.0\n",
        "\n",
        "# Install LangChain ecosystem\n",
        "!pip install -q -U \\\n",
        "    langchain>=0.2.0 \\\n",
        "    langchain-core>=0.2.0 \\\n",
        "    langchain-community>=0.2.0 \\\n",
        "    langchain-text-splitters>=0.2.0\n",
        "\n",
        "# Install document processing\n",
        "!pip install -q -U \\\n",
        "    pymupdf>=1.24.0 \\\n",
        "    python-docx>=1.1.0\n",
        "\n",
        "# Install CLI tools\n",
        "!pip install -q -U \\\n",
        "    typer[all]>=0.9.0 \\\n",
        "    rich>=13.0.0 \\\n",
        "    pyyaml>=6.0\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify_install"
      },
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "import transformers\n",
        "import peft\n",
        "import trl\n",
        "import langchain\n",
        "\n",
        "print(f\"Transformers: {transformers.__version__}\")\n",
        "print(f\"PEFT: {peft.__version__}\")\n",
        "print(f\"TRL: {trl.__version__}\")\n",
        "print(f\"LangChain: {langchain.__version__}\")\n",
        "print(\"\\n‚úÖ All libraries ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/ravidsun/llm-finetune.git\n",
        "%cd llm-finetune\n",
        "\n",
        "# Install package\n",
        "!pip install -q -e .\n",
        "\n",
        "# Verify CLI\n",
        "!python -m finetune_project --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Mount Google Drive (Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p /content/drive/MyDrive/llm-finetune/data\n",
        "!mkdir -p /content/drive/MyDrive/llm-finetune/output\n",
        "!mkdir -p /content/drive/MyDrive/llm-finetune/configs\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auth"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Hugging Face token (REQUIRED)\n",
        "print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "# Login\n",
        "!huggingface-cli login --token {hf_token}\n",
        "\n",
        "print(\"\\n‚úÖ Authenticated with Hugging Face!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auth_optional"
      },
      "outputs": [],
      "source": [
        "# Optional: Anthropic API key (for QA generation)\n",
        "use_qa_generation = False  # Set to True if you want QA generation\n",
        "\n",
        "if use_qa_generation:\n",
        "    anthropic_key = getpass(\"Enter your Anthropic API key: \")\n",
        "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
        "    print(\"‚úÖ Anthropic API configured!\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è QA generation disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: Prepare Training Data\n",
        "\n",
        "Choose one of the options below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "option_a"
      },
      "source": [
        "### Option A: Upload Your Own Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_data"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create data directory\n",
        "!mkdir -p data\n",
        "\n",
        "# Upload files (JSON, JSONL, PDF, DOCX, TXT)\n",
        "print(\"Select your training data files...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move to data directory\n",
        "for filename in uploaded.keys():\n",
        "    !mv \"{filename}\" data/\n",
        "\n",
        "print(f\"\\n‚úÖ Uploaded {len(uploaded)} file(s)\")\n",
        "!ls -lh data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "option_b"
      },
      "source": [
        "### Option B: Use Sample Data (for testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_data"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Create sample dataset\n",
        "sample_data = [\n",
        "    {\"instruction\": \"What is machine learning?\", \"input\": \"\", \"output\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses algorithms to analyze data, identify patterns, and make decisions with minimal human intervention.\"},\n",
        "    {\"instruction\": \"Explain neural networks\", \"input\": \"\", \"output\": \"Neural networks are computing systems inspired by biological neural networks in animal brains. They consist of interconnected nodes (neurons) organized in layers that process and transform input data to produce outputs. Each connection has a weight that adjusts as learning proceeds.\"},\n",
        "    {\"instruction\": \"What is deep learning?\", \"input\": \"\", \"output\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks) to progressively extract higher-level features from raw input. It excels at tasks like image recognition, natural language processing, and speech recognition.\"},\n",
        "    {\"instruction\": \"Define supervised learning\", \"input\": \"\", \"output\": \"Supervised learning is a machine learning approach where the algorithm learns from labeled training data. The model is trained on input-output pairs and learns to map inputs to correct outputs, which it can then apply to new, unseen data.\"},\n",
        "    {\"instruction\": \"What is reinforcement learning?\", \"input\": \"\", \"output\": \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties for its actions and learns to maximize cumulative rewards over time through trial and error.\"},\n",
        "]\n",
        "\n",
        "# Save to file\n",
        "!mkdir -p data\n",
        "with open('data/sample.jsonl', 'w') as f:\n",
        "    for item in sample_data:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Created sample dataset with {len(sample_data)} examples\")\n",
        "!head -n 2 data/sample.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "option_c"
      },
      "source": [
        "### Option C: Download from URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [],
      "source": [
        "# Download dataset from URL\n",
        "dataset_url = \"YOUR_DATASET_URL_HERE\"  # Replace with your URL\n",
        "\n",
        "# Uncomment to download:\n",
        "# !mkdir -p data\n",
        "# !wget -O data/dataset.jsonl \"{dataset_url}\"\n",
        "# !ls -lh data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: Create Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_config"
      },
      "outputs": [],
      "source": [
        "# Configuration optimized for Colab T4 (15GB VRAM)\n",
        "config_yaml = \"\"\"\n",
        "model:\n",
        "  model_name: \"Qwen/Qwen2.5-7B-Instruct\"  # Or: unsloth/Llama-3.2-3B-Instruct\n",
        "  lora_rank: 16\n",
        "  lora_alpha: 32\n",
        "  lora_dropout: 0.05\n",
        "  target_modules:\n",
        "    - q_proj\n",
        "    - v_proj\n",
        "    - k_proj\n",
        "    - o_proj\n",
        "  use_qlora: true  # IMPORTANT: 4-bit quantization for Colab\n",
        "\n",
        "data:\n",
        "  input_path: \"data\"\n",
        "  output_path: \"processed_data\"\n",
        "  input_type: \"json\"  # json, pdf, docx, txt\n",
        "\n",
        "  langchain:\n",
        "    enabled: false\n",
        "    qa_generation_enabled: false\n",
        "\n",
        "  augmentation:\n",
        "    enabled: false\n",
        "\n",
        "training:\n",
        "  num_epochs: 3\n",
        "  per_device_train_batch_size: 1\n",
        "  gradient_accumulation_steps: 16\n",
        "\n",
        "  learning_rate: 2.0e-4\n",
        "  warmup_ratio: 0.03\n",
        "  lr_scheduler_type: \"cosine\"\n",
        "\n",
        "  max_seq_length: 512  # Reduced for Colab\n",
        "\n",
        "  gradient_checkpointing: true\n",
        "  fp16: false\n",
        "  bf16: false  # T4 doesn't support bf16\n",
        "\n",
        "  output_dir: \"output\"\n",
        "  save_strategy: \"epoch\"\n",
        "  save_total_limit: 2\n",
        "\n",
        "  logging_steps: 10\n",
        "  report_to: []\n",
        "\n",
        "  evaluation_strategy: \"no\"\n",
        "\"\"\"\n",
        "\n",
        "# Save config\n",
        "with open('config.yaml', 'w') as f:\n",
        "    f.write(config_yaml)\n",
        "\n",
        "print(\"‚úÖ Configuration created!\")\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_data"
      },
      "outputs": [],
      "source": [
        "# Prepare training data\n",
        "!python -m finetune_project prepare-data --config config.yaml\n",
        "\n",
        "# Check output\n",
        "!ls -lh processed_data/\n",
        "!head -n 2 processed_data/train.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "## Step 9: Train Model\n",
        "\n",
        "‚è±Ô∏è Training time: ~30-60 minutes for 1K samples on T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "!python -m finetune_project train --config config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10"
      },
      "source": [
        "## Step 10: Save Model to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_drive"
      },
      "outputs": [],
      "source": [
        "# Copy output to Google Drive\n",
        "!cp -r output /content/drive/MyDrive/llm-finetune/\n",
        "\n",
        "print(\"‚úÖ Model saved to Google Drive: /MyDrive/llm-finetune/output/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step11"
      },
      "source": [
        "## Step 11: Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Model name from config\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "adapter_path = \"output\"\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load base model\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "print(\"\\n‚úÖ Model loaded! Ready for testing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model"
      },
      "outputs": [],
      "source": [
        "# Test with a prompt\n",
        "def generate_response(prompt, max_tokens=256):\n",
        "    # Format prompt (adjust based on your training format)\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the response part\n",
        "    response = response.split(\"### Response:\")[-1].strip()\n",
        "    return response\n",
        "\n",
        "# Test\n",
        "test_prompts = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Explain neural networks\",\n",
        "    \"What is deep learning?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step12"
      },
      "source": [
        "## Step 12: Download Model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_model"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Zip and download\n",
        "!zip -r my-lora-adapter.zip output/\n",
        "files.download('my-lora-adapter.zip')\n",
        "\n",
        "print(\"‚úÖ Model downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step13"
      },
      "source": [
        "## Step 13: Upload to Hugging Face (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_hf"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Set your model name\n",
        "repo_name = \"your-username/your-model-name\"  # Change this!\n",
        "\n",
        "# Upload\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"output\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\",\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model uploaded to: https://huggingface.co/{repo_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Out of Memory?\n",
        "- Reduce `per_device_train_batch_size` to 1\n",
        "- Reduce `max_seq_length` to 256\n",
        "- Use smaller model: `unsloth/Llama-3.2-3B-Instruct`\n",
        "\n",
        "### Training too slow?\n",
        "- Increase `per_device_train_batch_size` if VRAM allows\n",
        "- Reduce `max_seq_length`\n",
        "- Use Colab Pro+ with A100\n",
        "\n",
        "### Session disconnected?\n",
        "- Mount Google Drive and save checkpoints\n",
        "- Resume from checkpoint using `--resume-from-checkpoint`\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "üéâ **Congratulations!** You've fine-tuned your LLM!\n",
        "\n",
        "- Deploy your model: [PHASE5_DEPLOYMENT.md](https://github.com/ravidsun/llm-finetune/blob/master/docs/PHASE5_DEPLOYMENT.md)\n",
        "- Share on Hugging Face Hub\n",
        "- Integrate into your application\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Documentation](https://github.com/ravidsun/llm-finetune/tree/master/docs)\n",
        "- [GitHub Repo](https://github.com/ravidsun/llm-finetune)\n",
        "- [Colab Guide](https://github.com/ravidsun/llm-finetune/blob/master/docs/COLAB_GUIDE.md)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
