# Configuration for using existing JSONL files on Google Colab
# Optimized for Colab T4 GPU (15GB VRAM)

model:
  model_name: "Qwen/Qwen2.5-7B-Instruct"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
  use_qlora: true  # Required for Colab T4

data:
  # Use data/input for Colab (relative path)
  input_path: "data/input"
  output_path: "processed_data"
  input_type: "json"

  langchain:
    enabled: false
    qa_generation_enabled: false

  augmentation:
    enabled: false  # Set true if you want to augment your data

training:
  num_epochs: 3

  # Colab T4 optimized settings
  per_device_train_batch_size: 1  # Small batch for T4
  gradient_accumulation_steps: 16  # Effective batch = 16

  learning_rate: 2.0e-4
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"

  max_seq_length: 512  # Reduced for Colab

  # Memory optimization for T4
  gradient_checkpointing: true
  fp16: false
  bf16: false  # T4 doesn't support bf16

  output_dir: "output"
  save_strategy: "epoch"  # Save less frequently on Colab
  save_total_limit: 2     # Keep only 2 checkpoints

  logging_steps: 10
  report_to: []  # or ["tensorboard"] or ["wandb"]

  evaluation_strategy: "no"

# COLAB USAGE:
# 1. Create input directory and upload your JSONL file:
#    !mkdir -p data/input
#    from google.colab import files
#    uploaded = files.upload()
#    # Move uploaded file to input directory
#    !mv *.jsonl data/input/train.jsonl
#
# 2. Mount Google Drive (IMPORTANT for persistence):
#    from google.colab import drive
#    drive.mount('/content/drive')
#
# 3. Prepare data:
#    !python -m finetune_project prepare-data --config configs/existing_jsonl_colab.yaml
#
# 4. Train:
#    !python -m finetune_project train --config configs/existing_jsonl_colab.yaml
#
# 5. Save to Drive:
#    !cp -r output /content/drive/MyDrive/llm-finetune/
